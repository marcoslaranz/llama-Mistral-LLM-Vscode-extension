# Mistral-LLM-Ext README

Use Ollama to run Mistral LLM models in Visual Studio Code.

**Enjoy!**

## Requirements

To use this extension, you must have the following installed and configured on your machine:

1. **Ollama**  
   - Download and install Ollama from [https://ollama.com/download](https://ollama.com/download).
   - Start the Ollama service before using the extension.

2. **Mistral Model**  
   - Pull the Mistral model in Ollama by running:
     ```
     ollama pull mistral
     ```

3. **Visual Studio Code**  
   - Make sure you are running a compatible version of VS Code (see `engines.vscode` in `package.json`).

4. **This Extension**  
   - Install this extension from the VS Code Marketplace or from a `.vsix` file.